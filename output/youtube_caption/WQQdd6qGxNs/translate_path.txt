我想感谢组织者选择这篇论文获得这个奖项，这非常棒。同时，我也想感谢我出色的合著者和合作者Oral Vineel和Qule，他们刚才就站在你面前。你现在看到的是一张图片，一张来自10年前类似演讲的截图，那是在2014年蒙特利尔的新RPS会议上。那是一个更加纯真的时代，照片中展示了我们当时的样子，这是“之前”，顺便说一下，这是“之后”，现在我们更有经验了，希望也更明智了。但在这里，我想稍微谈谈这项工作本身，也许是对它进行一个10年的回顾，因为这项工作中的许多事情是正确的，但也有一些不是那么正确，我们可以回顾它们，看看发生了什么，以及它是如何逐渐发展到我们今天的。让我们从谈论我们做了什么开始，我们将通过展示10年前相同演讲的幻灯片来做到这一点。但我们所做的总结如下三个要点：它是一个基于文本训练的自回归模型，它是一个大型神经网络，它是一个大型数据集，就是这样。现在让我们更深入地探讨一下细节。

这是10年前的幻灯片，不算太差。我们在这里说的是，如果你有一个包含10层的神经网络，那么它可以在几分之一秒内完成人类能做的任何事情。为什么我们要强调人类能在几分之一秒内完成的事情？为什么特别强调这一点？如果你相信深度学习的教条，即人工神经元和生物神经元相似或至少没有太大不同，并且你相信真实的神经元是缓慢的，那么我们能快速完成的任何事情，我的意思是整个人类，甚至只是世界上一个能在一瞬间完成某项任务的人，那么一个10层的神经网络也能做到。我们专注于10层神经网络，因为这是当时我们知道如何训练的神经网络。

另一个演讲中的幻灯片，上面写着我们的主要想法，你可能会认出两件事，或者至少认出一件事，那就是这里正在进行某种自回归。这张幻灯片真正说的是，如果你有一个自回归模型，并且它足够好地预测下一个标记，那么它实际上会抓住并掌握接下来出现的序列的正确分布。这是一个相对较新的概念，虽然它不是有史以来第一个自回归神经网络，但我可以说它是我们真正相信如果训练得当就能得到你想要的东西的第一个自回归神经网络。

我想展示一些你们很多人可能从未见过的古老历史，它叫做LSTM。对于不熟悉的人来说，LSTM是Transformer出现之前可怜的深度学习研究人员所做的事情，它基本上是一个旋转了90度的ResNet。这是LSTM，它出现在Transformer之前，有点像稍微复杂一点的ResNet。你可以看到你的积分器，现在被称为残差流，但你有一些乘法在进行，它有点复杂，但这就是我们当时所做的。

我想强调的另一个来自旧演讲的酷炫特点是我们使用了并行化，但不仅仅是任何并行化，我们使用了流水线技术，正如这张幻灯片所示，每层一个GPU。现在我们知道流水线技术并不明智，但我们当时并不那么明智，所以我们使用了它，并使用八个GPU获得了3.5倍的速度提升。

在某种意义上，演讲中最后的结论幻灯片是最重要的幻灯片，因为它阐明了可以说是缩放假设的开始：如果你有一个非常大的数据集，并训练一个非常大的神经网络，那么成功是有保证的。如果你慷慨地看待这一点，可以说这确实是我们一直在经历的事情。

我想提到的另一个想法是，我声称这个想法真正经受住了时间的考验，它是部署的核心思想，即连接主义的思想。这个想法是，如果你允许自己相信人工神经元有点像生物神经元，那么它就会给你信心，让你相信非常大的神经网络不需要达到人类大脑的规模，它们可能稍微小一点，但你可以配置它们来做人类能做的几乎所有事情。

我声称这导致了预训练时代的到来，预训练时代是我们所说的GPT-2模型、GPT-3模型、缩放定律，我想特别指出我的前合作者Alec Radford、Jared Kaplan和Dario Amodei，他们真正推动了这项工作。这导致了预训练时代的到来，这是我们今天看到的所有进展的驱动力。

但预训练将毫无疑问地结束，为什么它会结束？因为尽管计算机通过更好的硬件、更好的算法和逻辑集群不断增长，所有这些都增加了你的计算能力，但数据并没有增长，因为我们只有一个互联网。你可以甚至说数据是AI的化石燃料，它被某种方式创造出来，现在我们使用它，我们已经达到了数据峰值，不会再有更多的数据了。

在这里，我将稍微自由地推测一下接下来会发生什么，实际上我不需要推测，因为很多人也在推测，我会提到他们的推测。你可能听说过“代理”这个词，人们觉得代理是未来的方向，更具体但也有些模糊的是合成数据，弄清楚合成数据是什么是一个巨大的挑战，我确信不同的人在那里都有各种有趣的进展。

我想提到生物学中的另一个例子，我认为这真的很酷。多年前在这个会议上，我看到有人展示了一张图表，显示了哺乳动物的身体大小与大脑大小之间的关系，这张图表显示了一个非常紧密的关系。我对此图表感到好奇，并在Google上搜索了这个图表，发现了一个有趣的图像，显示了不同哺乳动物的脑体比例，包括非人类灵长类动物和人类近亲。

我想稍微谈谈更长远的事情，我们都在朝着什么方向前进？我们正在取得惊人的进展，对于那些在10年前就在这个领域的人来说，你们还记得当时的一切是多么无能，是的，即使你可以说当然学习还在继续，但看到这一切仍然令人难以置信。

我想稍微谈谈超级智能，因为这显然是这个领域的发展方向。超级智能将与我们现在拥有的东西在质量上有所不同，我的目标是在接下来的几分钟内给你一些具体的直觉，让你自己能够推理它。

现在我们有了令人难以置信的语言模型和令人难以置信的聊天机器人，它们甚至可以做一些事情，但它们也有点奇怪地不可靠，当它们在评估中表现出显著的超人性能时，它们也会感到困惑，这真的不清楚如何调和这一点。但最终，这些系统实际上将以一种真实的方式成为代理，而现在的系统在任何有意义的方面都不是代理，它们只是非常非常轻微的代理，只是开始。

我想提到关于推理的一点是，一个推理的系统，它推理得越多，就越不可预测，它推理得越多，就越不可预测。我们一直使用的所有深度学习都是非常可预测的，因为如果你一直在复制人类的直觉，它就像是一种本能反应。

我们最终将不得不处理那些非常不可预测的AI系统，它们将从有限的数据中理解事物，它们不会感到困惑，所有这些都是真正的限制，我并不是说如何或何时，我说的是它将会发生，当所有这些事情与自我意识一起发生时，因为我们为什么不能有自我意识呢？自我意识是有用的，它是我们自己世界模型的一部分，当所有这些事情结合在一起时，我们将拥有具有今天不存在的大不相同品质和属性的系统。

当然，它们将拥有令人难以置信和惊人的能力，但与这些系统相关的问题，我将把它留作一个练习，想象一下，这与我们过去的情况非常不同，我可以说，预测未来真的不可能，各种各样的事情都是可能的，但在这个令人振奋的音符上，我将结束，非常感谢。

在2024年，你是否认为还有其他生物结构是人类认知的一部分，值得以类似的方式探索或你感兴趣的？我的回答是，如果有人有特定的见解，认为我们都在做非常愚蠢的事情，因为显然大脑做了一些我们没有做的事情，那么他们应该追求它。我个人不认为，这取决于你看的抽象层次，也许我会这样回答，有很多愿望去创造生物启发的AI，你可以在某种程度上认为生物启发的AI非常成功，但另一方面，生物启发非常非常非常有限，只是使用神经元，这是生物启发的全部范围，更详细的生物启发一直很难找到，但我不排除它，我认为如果有人有特殊的见解，他们可能会看到一些东西，那将是有用的。

关于自动修正的问题，你提到推理可能是未来建模的核心方面之一，也许是一个区分因素，我们在一些海报会议上看到的是，今天的模型中的幻觉是我们分析的方式，也许你纠正我，你是这方面的专家，但我们分析模型是否幻觉的方式是使用统计分析，比如某些标准差或远离平均值的程度，在未来，你认为一个具有推理能力的模型能够自我修正，这将是未来模型的核心特征，这样就不会有那么多幻觉，因为模型将能够推理并理解何时发生幻觉，这个问题有意义吗？是的，答案也是肯定的，我认为你描述的情况非常有可能。

我非常喜欢结尾，神秘地省略了它们是否会取代我们，或者它们是否更优越，它们是否需要权利，这是一个新物种的智人 spawned 智能，所以也许它们需要，我认为 RL 的人认为我们需要为这些东西争取权利，我有一个 UNR 问题，你如何为人类创造正确的激励机制，以一种给予它们与我们作为智人相同的自由的方式来创造它们？我认为这些问题是人们应该更多反思的，但关于我们应该创建什么样的激励结构的问题，我不觉得我有信心回答这样的问题，因为它就像你在谈论创建某种自上而下的结构，政府的东西，我不知道，它可能是加密货币，我知道那些东西，我不觉得我是评论加密货币的合适人选，但你知道，有可能，你描述的情况确实会发生，如果我们有 AIS，它们只是想与我们共存，并且也有权利，也许那将是好的，但我不确定，我认为事情非常不可预测，我犹豫评论，但我鼓励推测。

谢谢你的精彩演讲，我的名字是 Shalev Liit，来自多伦多大学，与 Sheila 一起工作，感谢你做的工作，我想问，你认为 llms 是否能泛化多跳推理，分布外？好的，这个问题假设答案是是或否，但问题不应该用是或否来回答，因为什么是分布外泛化，什么是分布内泛化，因为这是一个测试时间的演讲，我会说很久以前，在人们使用深度学习之前，他们使用的是字符串匹配 engrams 进行机器翻译，人们使用统计短语表，你能想象他们有成千上万的复杂性，这真的是难以想象的，那时泛化意味着它是否与数据集中的相同措辞不同，现在我们可能会说，我的模型在数学竞赛中取得了高分，但也许互联网上的某些论坛讨论了相同的想法，因此它被记住了，好吧，你可以说它可能在分布内，可能是记忆，但我也认为我们对什么是泛化的标准已经大大提高了，非常显著，如果你一直关注的话，所以我认为答案是某种程度上，可能不像人类那样好，我认为人类泛化得更好，但同时它们肯定在某种程度上泛化分布外，我希望这是一个有用的拓扑答案。

不幸的是，我们没有时间继续这个环节了，我感觉我们可以再继续六个小时，但非常感谢 Ilia 的演讲，谢谢大家。