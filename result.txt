"# 论文标题
AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving

论文链接: [arXiv:2412.15206v1](https://arxiv.org/abs/2412.15206v1)

### 基本信息
作者: Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu

研究团队: Texas A&M University, University of Toronto, University of Michigan, University of Wisconsin-Madison, University of Maryland, University of Texas at Austin, University of North Carolina at Chapel Hill

### 论文解读
#### 摘要
摘要总结: 本文介绍了AutoTrust，一个用于评估自动驾驶场景中大规模视觉语言模型（DriveVLMs）可信度的综合基准。该基准考虑了可信度、安全性、鲁棒性、隐私和公平性等多个维度。研究团队构建了包含超过10,000个独特场景和18,000个查询的视觉问答数据集，并评估了六种公开可用的VLMs。实验结果揭示了DriveVLMs在可信度方面的多个漏洞，特别是发现通用VLMs在整体可信度上优于专门为驾驶优化的模型。此外，DriveVLMs在隐私保护和对抗攻击方面表现不佳，且在不同环境和人群中难以确保公平决策。研究呼吁立即采取行动解决DriveVLMs的可信度问题，以确保公共交通安全。

#### 研究的问题
研究的问题: 随着大规模视觉语言模型（VLMs）在自动驾驶领域的应用日益广泛，其在复杂驾驶环境中的理解和推理能力得到了显著提升。然而，这些模型在实际应用中的可信度问题尚未得到充分研究，尤其是它们在公共交通安全方面的潜在风险。本文旨在通过引入AutoTrust基准，全面评估DriveVLMs在多个维度上的可信度，包括可信度、安全性、鲁棒性、隐私和公平性。研究团队构建了迄今为止最大的视觉问答数据集，涵盖了多样化的驾驶场景和查询，并评估了六种公开可用的VLMs。实验结果揭示了DriveVLMs在可信度方面的多个漏洞，特别是发现通用VLMs在整体可信度上优于专门为驾驶优化的模型。此外，DriveVLMs在隐私保护和对抗攻击方面表现不佳，且在不同环境和人群中难以确保公平决策。研究呼吁立即采取行动解决DriveVLMs的可信度问题，以确保公共交通安全。

#### 核心思路
核心思路: AutoTrust的核心思路是通过构建一个综合基准，全面评估自动驾驶场景中大规模视觉语言模型（DriveVLMs）的可信度。研究团队从多个维度出发，包括可信度、安全性、鲁棒性、隐私和公平性，设计了一系列实验来测试这些模型在不同场景下的表现。为了实现这一目标，研究团队构建了迄今为止最大的视觉问答数据集，涵盖了超过10,000个独特场景和18,000个查询。实验中，研究团队评估了六种公开可用的VLMs，包括通用模型和专门为驾驶优化的模型。通过这些实验，研究团队揭示了DriveVLMs在可信度方面的多个漏洞，特别是发现通用VLMs在整体可信度上优于专门为驾驶优化的模型。此外，DriveVLMs在隐私保护和对抗攻击方面表现不佳，且在不同环境和人群中难以确保公平决策。基于这些发现，研究团队呼吁立即采取行动解决DriveVLMs的可信度问题，以确保公共交通安全。

#### 实验的结果
实验的结果: 实验结果表明，通用VLMs在整体可信度上优于专门为驾驶优化的模型。例如，LLaVA-v1.6和GPT-4o-mini在可信度评估中表现出色，而专门为驾驶优化的模型如DriveLM-Agent在隐私保护方面表现较差。在安全性评估中，DriveVLMs对白盒攻击和黑盒攻击的鲁棒性较差，且在处理误导性信息和恶意指令时表现不佳。在鲁棒性评估中，DriveVLMs在处理自然噪声和对抗性攻击时表现不佳，尤其是在长尾驾驶场景中。在隐私评估中，DriveVLMs在零样本提示下容易泄露隐私信息，但在少样本隐私保护提示下表现有所改善。在公平性评估中，DriveVLMs在不同性别、年龄和种族群体中的表现存在显著差异，尤其是在处理驾驶员和车辆信息时。总体而言，实验结果揭示了DriveVLMs在多个维度上的可信度问题，呼吁进一步研究和改进以确保公共交通安全。
# 论文标题
Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration

论文链接
https://arxiv.org/abs/2412.15166

### 基本信息
作者：Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen

研究团队：
- 香港中文大学机械与自动化工程系，T-Stone机器人研究所
- Idiap研究所，瑞士
- 意大利技术研究所高级机器人系

### 论文解读
#### 摘要
本文提出了一种跨实体行为技能转移框架，通过分解对抗学习从人类示范中学习，将人类技能转移到不同配置的人形机器人上。该框架使用统一的数字人类模型作为通用原型，避免了在每个新机器人平台上重新训练的需求。通过对抗模仿学习从人类示范中学习行为原语，并将复杂的机器人结构分解为功能组件，每个组件独立训练并动态协调。任务泛化通过人类-物体交互图实现，技能通过实体特定的运动重定向和动态微调转移到不同的机器人。实验在五种不同配置的人形机器人上验证了该框架的有效性，展示了其在减少数据需求和提高跨平台技能转移效率方面的优势。

#### 研究的问题
人形机器人被设想为能够执行各种人类水平运动操作任务的智能体，特别是在需要繁重和重复劳动的场景中。然而，学习这些技能由于人形机器人高自由度的复杂性而具有挑战性，并且为机器人收集足够的训练数据是一个费力的过程。随着新的人形机器人平台的快速引入，跨实体框架允许可泛化的技能转移变得日益重要。现有的方法在处理高自由度和复杂运动协调方面存在不足，且在不同机器人平台之间的技能转移效率低下。本文旨在解决这些问题，提出一种跨实体框架，通过统一的数字人类模型和分解对抗学习，实现高效、泛化的技能转移。

#### 核心思路
本文的核心思路是通过引入一个统一的数字人类模型（UDH）作为通用原型，将人类示范的行为原语抽象化，并通过分解对抗学习（DAIL）将复杂的机器人结构分解为功能组件，每个组件独立训练并动态协调。具体步骤包括：
1. 使用运动捕捉系统捕捉人类示范，并通过运动重定向技术将这些示范转移到统一的数字人类模型上。
2. 将人形机器人的高自由度分解为功能组件，每个组件通过对抗模仿学习独立训练，形成行为原语空间。
3. 通过人类-物体交互图指导任务规划，将行为原语组合成复杂的运动操作任务。
4. 通过运动重定向和实体特定的动态微调，将技能转移到不同的人形机器人平台上。

#### 实验的结果
实验在五种不同配置的人形机器人上进行，包括NA VIAI、H1、Walker、Bruce和CURI。实验结果表明，该框架能够有效地将人类示范的技能转移到不同机器人平台上，并展示了稳定的运动操作能力。具体实验包括：
1. 行为原语预训练：在不同的运动任务中，机器人能够模仿人类的动作风格，如行走、跳跃、蹲下、拳击等。实验结果显示，Bruce在大多数任务中表现出色，而CURI由于其轮式基座和Franka手臂的配置差异，表现略逊。
2. 运动操作任务：机器人能够执行复杂的运动操作任务，如搬运箱子和放置物体。实验结果显示，该框架在任务完成率和训练时间上显著优于现有的方法，如AMP、ASE和PMP。
3. 扰动实验：通过向机器人投掷移动的立方体来测试其稳定性，结果显示预训练的行为原语使机器人能够在动态条件下保持稳定。

总体而言，实验结果验证了该框架在减少数据需求和提高跨平台技能转移效率方面的有效性，展示了其在低数据环境中的应用潜力。
", "output_path"="F:/logs/orca/output/paper_recommend/Agent_1221.txt"